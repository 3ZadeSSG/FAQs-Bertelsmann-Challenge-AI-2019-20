
**I am having some trouble understanding backpropagation when training the neural net.**

Resources:
- http://neuralnetworksanddeeplearning.com/chap2.html
- https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec

**What's the perceptron algorithm?**

Resources:
- https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53

**How to find the optimal learning rate?**

This paper by Leslie Smith is a great resource in finding the optimal learning rate: https://arxiv.org/abs/1803.09820 . You can find implementation of this paper in this blog:https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0

**What is cross entropy Loss?**

Resources:
-https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a

**What is bias?**

Resources:
-https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
-https://www.youtube.com/watch?v=aircAruvnKk

**What is Gradient Descent?**

Resources:
-https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s

**In softmax function why do we take exponential?**

Resources:
-https://datascience.stackexchange.com/questions/23159/in-softmax-classifier-why-use-exp-function-to-do-normalization


        
